---
title: "Modélisation du nombre de consultations en médecine de ville en tenant compte des 
caractéristiques des villes et de la spatialisation : approche par modèle mixte"
author: "Groupe 25"
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: true
    toc: true
    toc_depth: 3
header-includes:
  - \usepackage[french]{babel}
  - \usepackage[T1]{fontenc}
  - \usepackage[utf8]{inputenc}
  - \usepackage{etoolbox} # Pour modifier le comportement de \tableofcontents
  - \pretocmd{\tableofcontents}{\clearpage}{}{} # Ajout d'un saut de page avant le sommaire
geometry: a4paper
fontsize: 12pt
linestretch: 1.5
---

```{r fig.align='center', fig.height=6, fig.width=6}
# Chargement des packages

library(stringi)
library(dplyr)
library(tidyverse)
library(summarytools)
library(gridExtra)
library(purrr)
library(skimr)

# Importation des données de démonstration

demo <- read.csv("data/demof2.csv", sep = ";", dec=",")
View(demo)
str(demo)
names(demo)[names(demo) == "Libellé"] <- "libelle_maj"

## Fonction prenant en entrée un base et nettoie les noms des colonnes

nettoyer_noms_colonnes <- function(data){
  names(data) <- names(data) %>%
    stri_trans_general("Latin-ASCII") %>% # Suppression des accents
    gsub("\\s+", "_", .) %>% # Remplacement des espaces par des underscores
    gsub("\\.+", "_", .) %>% # Remplacement des points par des underscores
    tolower() # Conversion en minuscules
  return (data) 
}

## Nettoyage des colonnes de la base demo
demo <- nettoyer_noms_colonnes(demo)
names(demo)


# Fusion des bases et création des varaiables

## Importation de la base generalise
generalise <- read.csv("data/generalise.csv", sep=";")
str(generalise)

## Importation de la base pour les lon et lat manquantes
donnees_manquantes <- read.csv(
  "data/communes_manquantes_latitudes_longitudes.csv", sep=";", dec=".")
str(donnees_manquantes)

donnees_manquantes$longitude <- donnees_manquantes$longitude %>%
  str_replace_all(",", "") %>%  # Supprime les virgules
  as.numeric()

## Nettoyage dans les noms des colonnes
generalise <- nettoyer_noms_colonnes(generalise)
donnees_manquantes <- nettoyer_noms_colonnes(donnees_manquantes)

## Fusion des bases
data <- demo %>% 
  inner_join(generalise, by ="code") %>%
  left_join(donnees_manquantes, by = "code") %>%
  mutate(
    longitude = ifelse(is.na(longitude.x), longitude.y, longitude.x),
    latitude = ifelse(is.na(latitude.x), latitude.y, latitude.x)
  ) %>%
  select(-longitude.x, -longitude.y, -latitude.x, -latitude.y)


nrow(demo)
nrow(generalise)
nrow(data)

## Filtrons les communes n'appartenant pas au département 97
data <- data %>% filter(departement != 97)



## Création de la variable taux de visites
data <- data %>% 
  mutate(taux_visites = nb_visite/population_municipale_2021_x)

## Création de la variabe taux de visites pour les plus de 19 ans
data <- data %>%
  mutate(pop_19_ans_ou_plus = pop_15_ans_ou_plus - pop_15_19_ans,
       taux_visites_19_ans_ou_plus = nb_visite / pop_19_ans_ou_plus)

summary(data$taux_visites)
summary(data$taux_visites_19_ans_ou_plus)

skim(data)

## Exportation de la base finale 
write.csv(data, "data/data.csv", row.names = FALSE)

## Statistiques descriptives sur le nombre de visite

summary(data$nb_visite)

ggplot(data) +
  aes(x = nb_visite) +
  geom_histogram(bins = 30L, fill = "gray") +
  theme_minimal() +
  ggtitle(label = "Distribution du nombre de visites par commune") +
  ylab("") +
  xlab("")

```


# Introduction

# Présentation du contexte

## Intérêt de l'étude

## Cadre conceptuel de l'étude

## Présentation des données

# Méthodologie

# Analyse des résultats

## Analyse descriptive


Dans cette partie, nous allons réaliser quelques statistiques descriptives sur nos données. 


### Analyse univariée


### Analyse bivariée 

Nous allons ici, voir s'il y a un lien à priori entre le taux de consultations et certaines de nos variables explicatives. Ainsi, nous avons calculé la corrélation de Pearson pour évaluer le lien linéaire entre le taux de consulation et des variables telles que la population totale, la part des personnes agées (75 ans et plus), la part de quelques CSP (ouvriers et retraités). Les résultats sont consignées dans le tableau suivant : 

```{r}
library(dplyr)
library(knitr)

data$taux_visites_19_ans_ou_plus <- as.numeric(data$taux_visites_19_ans_ou_plus)
data$population_municipale_2021_x <- as.numeric(data$population_municipale_2021_x)
data$part_des_pers_agees_de_75_ans_ou_2021 <- as.numeric(data$part_des_pers_agees_de_75_ans_ou_2021)
data$population_de_15_ans_ou_selon_la_csp_2021_retraites <- as.numeric(data$population_de_15_ans_ou_selon_la_csp_2021_retraites)
data$population_de_15_ans_ou_selon_la_csp_2021_ouvriers <- as.numeric(data$population_de_15_ans_ou_selon_la_csp_2021_ouvriers)

# Liste des variables à tester avec taux_de_consultation
variables <- c("population_municipale_2021_x", "part_des_pers_agees_de_75_ans_ou_2021", 
               "population_de_15_ans_ou_selon_la_csp_2021_retraites", "population_de_15_ans_ou_selon_la_csp_2021_ouvriers")

# Initialiser un tableau pour stocker les résultats
resultats <- data.frame(Variable = character(), Correlation = numeric(), P_value = numeric())

# Calculer la corrélation pour chaque variable et tester la significativité
for (var in variables) {
  test <- cor.test(data$taux_visites_19_ans_ou_plus, data[[var]], method = "pearson")
  resultats <- rbind(resultats, data.frame(
    Variable = var,
    Correlation = test$estimate,
    P_value = test$p.value
  ))
}

# Format du tableau avec la significativité
resultats$Significatif <- ifelse(resultats$P_value < 0.05, "Oui", "Non")

# Afficher le tableau dans le RMarkdown
kable(resultats, caption = "Corrélations de Pearson entre le taux de consultation et les autres variables")
```


Les résultats nous montrent que le taux de consultation est positivement corrélé à la population ainsi qu'à celle de plus de 15 ans. Cependant la corrélation est faible. Par ailleurs, la corrélation est négative avec la part des personnes agées de plus de 75 ans. Cela dit, plus la part des plus de 75 ans augmente moins est le taux de consultations dans une commune. Cela peut vouloir dire que les personnes de plus de 75 ans sont ceux qui ne se consultent pas assez. On peut voir cela à partir de ce graphique ci dessous. 
```{r fig.align='center', fig.height=6, fig.width=6}
# Charger la bibliothèque ggplot2
library(ggplot2)


# Utilisation du jeu de données mtcars
# Variables : Chevaux-vapeur (hp) vs Miles par gallon (mpg)
ggplot(data = data, aes(x = part_des_pers_agees_de_75_ans_ou_2021 , y = taux_visites_19_ans_ou_plus)) +
  geom_point(color = "blue", size = 3) +          # Points bleus
  geom_smooth(method = "lm", se = TRUE, color = "red") +  # Droite de tendance (modèle linéaire)
  labs(
    title = "Relation entre taux de consultations et part des plus de 75 ans",
    x = "Part des plus de 75 ans",
    y = "Taux de consultation"
  ) +
  theme_minimal()

```
### Autocorrélation 

L’autocorrélation spatiale est une mesure essentielle pour analyser la dépendance entre des observations géographiques. Dans notre étude nos données sont des données portant sur des communes. Ainsi il peut exister une dépendance entre nos taux de consultations du fait de la proximité des communes ou de l'appartenance à un même département ou région. Ainsi nous allons mesurer cette dépendance en évaluant l'autocorrélation spatiale. Dans ce contexte, **l’indice de Moran** est largement utilisé pour quantifier cette dépendance en fournissant une mesure globale de l’autocorrélation spatiale.

#### Définition de l’indice de Moran
L’indice de Moran (\(I\)) évalue la similitude des valeurs d’une variable entre différentes entités géographiques (par exemple, des communes) en fonction de leur proximité spatiale. Il se base sur la matrice de poids spatiale (\(W\)), qui définit les relations entre ces entités.

#### Formule de l’indice de Moran
La formule mathématique de l’indice de Moran est la suivante :

\[
I = \frac{n}{\sum_{i=1}^n \sum_{j=1}^n w_{ij}} \cdot \frac{\sum_{i=1}^n \sum_{j=1}^n w_{ij} (x_i - \bar{x})(x_j - \bar{x})}{\sum_{i=1}^n (x_i - \bar{x})^2}
\]

Où :
- \(n\) : Nombre total d’entités spatiales (Ici, le nombre de communes).
- \(x_i, x_j\) : Valeurs observées de la variable pour les entités \(i\) et \(j\) (Ici le taux de consultations)
- \(ar{x}\) : Moyenne de la variable \(x\).
- \(w_{ij}\) : Poids spatial définissant la relation entre \(i\) et \(j\).

La matrice de \(W\) peut être constuit sur la base du voisinage entre les deux communes ou soit de la distance entre les deux communes. Dans le premier cas alors \(w_{ij}\) 
\(w_{ij} = 1\) si \(i\) et \(j\) sont voisins et \(w_{ij} = 0\) sinon. Dans le second cas \(w_{ij} = d_{ij}\). Nous allons dans notre cas utiliser une matrice de poids basée sur la distance, notamment celle d'Haversine. 

#### Matrice de poids basée sur la distance de Haversine

##### Définition de la distance de Haversine
La distance de Haversine est une mesure de la distance entre deux points sur une sphère, basée sur leurs coordonnées géographiques (\(latitude\) et \(longitude\)). Elle est particulièrement utile pour les données géographiques projetées sur une surface sphérique, comme la Terre.

#### Formule de la distance de Haversine
\[
 d_{ij} = 2r \cdot \arcsin\left(\sqrt{\sin^2\left(\frac{\phi_j - \phi_i}{2}\right) + \cos(\phi_i)\cos(\phi_j)\sin^2\left(\frac{\lambda_j - \lambda_i}{2}\right)}\right)
\]

Où :
- \(r\) : Rayon de la Terre (environ 6371 km).
- \(\phi_i, \phi_j\) : Latitudes des points \(i\) et \(j\) (en radians).
- \(\lambda_i, \lambda_j\) : Longitudes des points \(i\) et \(j\) (en radians).

#### Construction de la matrice de poids

Pour construire la matrice de poids, nous avons alors suivi ces étapes. *

1. Calculer les distances de Haversine entre chaque paire d’entités.
2. Définir un seuil de distance maximale (\(d_{max}\)) :
   - Si \(d_{ij} < d_{max}\), \(w_{ij} = \frac{1}{d_{ij}}\).
   - Sinon, \(w_{ij} = 0\).
3. Normaliser les poids pour que chaque ligne de la matrice ait une somme égale à 1 :
\[
 w_{ij}^{norm} = \frac{w_{ij}}{\sum_{j} w_{ij}}.
\]

```{r}
# Charger les bibliothèques nécessaires
#install.packages("spdep")
#install.packages("geosphere")
library(spdep)
library(geosphere)

#  (latitude, longitude)
coords <- data.frame(
  lat = data$latitude,  
  lon = data$longitude
)

# Calcul des distances de Haversine (en mètres)
dist_matrix <- distm(coords, fun = distVincentySphere)

# Créer la matrice de poids (poids inverse de la distance)
weight_matrix <- 1 / dist_matrix
diag(weight_matrix) <- 0  # Pas de poids pour soi-même

# Créer l'objet de poids spatial
W <- mat2listw(weight_matrix)

# Exemple de valeurs (X_i) à analyser
values <- data$taux_visites_19_ans_ou_plus

# Calcul de l'Indice de Moran
moran_result <- moran.test(values, W)

# Afficher les résultats
print(moran_result)
```


Ainsi dans notre étude, nous avons trouvé un indice de Moran égale à 



# Discussion

# Conclusion

# Références bibliographiques

# Annexes
