---
header-includes:
   \usepackage{amsmath}
   \usepackage{geometry}
   \usepackage{pdfpages}
   \usepackage{fontspec}
   \usepackage{pdfpages}
   \usepackage{graphicx}
   \usepackage{amsmath}
   \usepackage{atbegshi}
   \usepackage{fancyhdr}
   \usepackage{tocloft}
   \usepackage{tcolorbox}
   \usepackage{xcolor}
   \definecolor{bleu}{RGB}{0,0,255}
   \usepackage{everypage}
   \usepackage{everypage}
   \usepackage{graphicx}
   \usepackage{fancyhdr}
   \pagestyle{fancy} 
   \definecolor{mybrown}{RGB}{139,69,19}
   \fancyhead[R]{}
output: 
  pdf_document: 
    number_sections: true
    latex_engine: xelatex
    fig_height: 4
    fig_width: 5
    fig_caption: true
    keep_tex: true
lang: fr
urlcolor: blue
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{=tex}
\setcounter{tocdepth}{5}                
\renewcommand\contentsname{\begin{center}\textcolor{brown}{Sommaire}\end{center}}
```


```{=tex}
\AtBeginShipout{
  \ifnum\value{page}=1\thispagestyle{empty}\fi}
```


```{=tex}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\fancyhead[L]{Elèves Ingénieurs}
\fancyhead[R]{\textcolor{brown}{@Alex, Ali, Richard \& Toussaint}}
\fancyfoot[C]{\thepage}
\fancyfoot[L]{Mars 2025}
\fancyfoot[R]{Projet Statistique}
```
```{=tex}
\AddEverypageHook{
  \ifnum\value{page}>1 
    \fancyhead[L]{Elèves Ingénieurs}
    \fancyhead[R]{\textcolor{brown}{@Alex, Ali, Richard \& Toussaint}}
    \fancyfoot[C]{\thepage}
    \fancyfoot[L]{Mars 2025}
    \fancyfoot[R]{Projet Statistique}
  \else
    \fancyhead[L]{} 
    \fancyhead[R]{}
    \fancyfoot[C]{}
    \fancyfoot[R]{}
  \fi
}
```

```{=latex}
\tableofcontents
```

\newpage
```{=tex}
\renewcommand\listtablename{\begin{center}\textcolor{brown}{Liste des Tableaux}\end{center}}
\renewcommand\listfigurename{\begin{center}\textcolor{brown}{Liste des Figures}\end{center}} 

\setlength{\cftfignumwidth}{3em}
\setlength{\cfttabnumwidth}{3em}
```

```{=latex}
\listoftables
```

\newpage

```{=latex}
\listoffigures
```

\newpage

```{r setup, include=FALSE, fig.align='center'}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
```

```{r}
# Chargement des packages
library(stringi)
library(dplyr)
library(knitr)
library(tidyverse)
library(summarytools)
library(gridExtra)
library(purrr)
library(skimr)
library(spdep)
library(geosphere)
```



```{r }
# Importation des données de démonstration

demo <- read.csv("../data/demof2.csv", sep = ";", dec=",")
#View(demo)
#str(demo)
names(demo)[names(demo) == "Libellé"] <- "libelle_maj"

## Fonction prenant en entrée un base et nettoie les noms des colonnes

nettoyer_noms_colonnes <- function(data){
  names(data) <- names(data) %>%
    stri_trans_general("Latin-ASCII") %>% # Suppression des accents
    gsub("\\s+", "_", .) %>% # Remplacement des espaces par des underscores
    gsub("\\.+", "_", .) %>% # Remplacement des points par des underscores
    tolower() # Conversion en minuscules
  return (data) 
}

## Nettoyage des colonnes de la base demo
demo <- nettoyer_noms_colonnes(demo)
#names(demo)


# Fusion des bases et création des varaiables

## Importation de la base generalise
generalise <- read.csv("../data/generalise.csv", sep=";")
#str(generalise)

## Importation de la base pour les lon et lat manquantes
donnees_manquantes <- read.csv(
  "../data/communes_manquantes_latitudes_longitudes.csv", sep=";", dec=".")
#str(donnees_manquantes)

donnees_manquantes$longitude <- donnees_manquantes$longitude %>%
  str_replace_all(",", "") %>%  # Supprime les virgules
  as.numeric()

## Nettoyage dans les noms des colonnes
generalise <- nettoyer_noms_colonnes(generalise)
donnees_manquantes <- nettoyer_noms_colonnes(donnees_manquantes)

## Fusion des bases
data <- demo %>% 
  inner_join(generalise, by ="code") %>%
  left_join(donnees_manquantes, by = "code") %>%
  mutate(
    longitude = ifelse(is.na(longitude.x), longitude.y, longitude.x),
    latitude = ifelse(is.na(latitude.x), latitude.y, latitude.x)
  ) %>%
  select(-longitude.x, -longitude.y, -latitude.x, -latitude.y)


#nrow(demo)
#nrow(generalise)
#nrow(data)

## Filtrons les communes n'appartenant pas au département 97
data <- data %>% filter(departement != 97)



## Création de la variable taux de visites
data <- data %>% 
  mutate(taux_visites = nb_visite/population_municipale_2021_x)

## Création de la variabe taux de visites pour les plus de 19 ans
data <- data %>%
  mutate(pop_19_ans_ou_plus = pop_15_ans_ou_plus - pop_15_19_ans,
       taux_visites_19_ans_ou_plus = nb_visite / pop_19_ans_ou_plus)

#summary(data$taux_visites)
#summary(data$taux_visites_19_ans_ou_plus)

#skim(data)




## Exportation de la base finale 
write.csv(data, "../data/data.csv", row.names = FALSE)

```


# Introduction

# Présentation du contexte

## Intérêt de l'étude

## Cadre conceptuel de l'étude

## Présentation des données


Les données que nous avons utilisées nous proviennent de ...

# Méthodologie

## Motivation
Les modèles linéaires généralisés à effets mixtes (GLMM) combinent :  

- Les caractéristiques des modèles linéaires généralisés (GLM) pour modéliser des variables non-normalement distribuées.  

- Les propriétés des modèles à effets mixtes pour gérer des données groupées ou hiérarchiques.  

## Modèles Linéaires Généralisés
Un GLM relie le prédicteur linéaire $\eta$ à la moyenne $\mu$ de la réponse à travers une fonction de lien $g$ :  
$$ g(\mu) = \eta = \beta_0 + \sum_{i=1}^m \beta_i x_i $$  
Les distributions possibles incluent :  

- **Normale** : Régression linéaire classique, avec lien identité.  

- **Binomiale** : Régression logistique pour données binaires, avec lien logit.  

- **Poisson** : Régression de Poisson pour données de comptage, avec lien logarithmique.  

### Régression Logistique
Modélise une réponse binaire ($y \sim B(n, p)$), où $p$ est la probabilité de succès :  
$$ P(y | n, p) = \binom{n}{y} p^y (1-p)^{n-y} $$  
La probabilité $p$ est reliée au prédicteur par la fonction logistique :  
$$ p = \frac{1}{1 + e^{-\eta}} \quad \text{où} \quad \eta = \beta_0 + \sum_{i=1}^m \beta_i x_i. $$  
Le log-vraisemblance est exprimé comme :  
$$ \ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left[ y_i \log{p_i} + (1-y_i) \log{(1-p_i)} \right]. $$  

### Régression de Poisson
Utilisée pour modéliser des données de comptage ($y \sim Pois(\lambda)$), où $\lambda$ est la moyenne et la variance :  
$$ P(y | \lambda) = \frac{\lambda^y}{y!} e^{-\lambda} $$  
Le lien logarithmique assure $\lambda > 0$ :  
$$ \log{\lambda} = \beta_0 + \sum_{i=1}^m \beta_i x_i $$  
L'espérance est $E[y] = \lambda$.  

## Modèles Linéaires Mixtes
Ces modèles ajoutent des termes d'effets aléatoires $\mathbf{Z} \mathbf{u}$ au prédicteur linéaire :  
$$ \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{Z} \mathbf{u} + \boldsymbol{\varepsilon}, $$  
avec :  

- $\mathbf{u} \sim N(\mathbf{0}, \mathbf{G})$, les effets aléatoires.  

- $\boldsymbol{\varepsilon} \sim N(\mathbf{0}, \mathbf{R})$, les résidus.  

La matrice de covariance totale est :  
$$ \mathrm{Var}(\mathbf{y}) = \mathbf{Z} \mathbf{G} \mathbf{Z}^T + \mathbf{R}. $$  
Les paramètres sont estimés par maximum de vraisemblance (ML) ou par vraisemblance restreinte (REML).  

## Modèles Linéaires Généralisés à Effets Mixtes (GLMM)
Un GLMM étend les GLM en intégrant des effets aléatoires :  
$$ g(\mu) = \mathbf{X} \boldsymbol{\beta} + \mathbf{Z} \mathbf{u}, $$  
où :  
- $g(\cdot)$ est la fonction de lien.  
- $\mathbf{u} \sim N(\mathbf{0}, \mathbf{G})$ est le vecteur d'effets aléatoires.  

Les paramètres sont estimés via des méthodes comme :  
- Approximations Laplaciennes.  
- Quadrature gaussienne adaptative.  
- Méthodes MCMC (chaînes de Markov Monte Carlo).  

## Prédictions et Simulations
Les GLMM permettent deux types de prédictions :  
- **Conditionnelles** : Basées sur les effets aléatoires spécifiques ($\mathbf{u}$).  
- **Marginales** : En intégrant sur les effets aléatoires.  

Les simulations utilisent des approches paramétriques pour évaluer la variabilité et tester les hypothèses. Une approche courante est le bootstrap paramétrique :  
1. Générer des données simulées basées sur les paramètres estimés.  
2. Réajuster le modèle pour chaque jeu de données simulé.  
3. Analyser la distribution des estimations obtenues.  

# Analyse des résultats

## Analyse descriptive


Dans cette partie, nous allons réaliser quelques statistiques descriptives sur nos données. 


### Analyse univariée

```{r }
## Statistiques descriptives sur le nombre de visite

summary(data$nb_visite)

```

L'analyse des statistiques descriptives sur le nombre de consultations annuelles de médecin généraliste entre 2018 et 2022 révèle une distribution fortement asymétrique à droite, avec une grande dispersion des données. La moyenne de 19130 consultations, nettement supérieure à la médiane de 9127, indique la présence de valeurs extrêmes tirant la distribution vers le haut. Cette asymétrie est confirmée par l'écart considérable entre le minimum de 1037 et le maximum de 765833 consultations par an.

La moitié des médecins généralistes effectuent entre 5993 et 17290 consultations annuellement, ce qui suggère une variabilité importante dans la charge de travail. La médiane de 9127 consultations par an, équivalant à environ 25 consultations par jour ouvrable, semble plus représentative de l'activité typique d'un médecin généraliste que la moyenne influencée par les valeurs extrêmes. Ces statistiques mettent en lumière la diversité des pratiques et des charges de travail parmi les médecins généralistes, avec potentiellement quelques cas atypiques présentant un volume de consultations exceptionnellement élevé. 

Le nombre de visites pouvant potentiellement être influencé par la taille de la commune et donc par sa population, nous avons éliminer cet effet en calculant le taux de consultations qui n'est autre que le nombre de consultations moyennes par personnes. 

```{r, fig.align='center', fig.cap= "Répartition du nombre et du taux de consultations"}
library(ggplot2)
library(patchwork)

# Premier graphique pour nb_visite
plot1 <- ggplot(data) +
  aes(x = nb_visite) +
  geom_histogram(bins = 30L, fill = "gray") +
  theme_minimal() +
  ylab("Nombre de consultations") +
  xlab("")

# Deuxième graphique pour taux_visites_19_ans_ou_plus
plot2 <- ggplot(data) +
  aes(x = taux_visites_19_ans_ou_plus) +
  geom_histogram(bins = 30L, fill = "gray") +
  theme_minimal() +
  ylab("Taux de consultations") +
  xlab("")

# Combinaison des deux graphiques
plot1 + plot2

```



### Analyse bivariée 

Nous allons ici, voir s'il y a un lien à priori entre le taux de consultation et certaines de nos variables explicatives. Ainsi, nous avons d'abord réalisé une analyse descriptive bivariée puis nous avons calculé la corrélation de Pearson pour évaluer le lien linéaire entre le taux de consulation et des variables telles que la population totale, la part des personnes agées (75 ans et plus), la part de quelques CSP (ouvriers et retraités).

#### Taux de consultation et population totale

```{r}
# Calculer les quantiles
quantiles <- quantile(data$population_municipale_2021_x, probs = c(1/3, 2/3), na.rm = TRUE)

# Créer des classes avec les bornes des intervalles
data_pop <- data %>%
  mutate(taille_commune = case_when(
    population_municipale_2021_x <= quantiles[1] ~ paste0("Petite (<= ", round(quantiles[1]), ")"),
    population_municipale_2021_x <= quantiles[2] ~ paste0("Moyenne (", round(quantiles[1] + 1), " - ", round(quantiles[2]), ")"),
    TRUE ~ paste0("Grande (> ", round(quantiles[2]), ")")
  )) %>%
  group_by(taille_commune) %>%
  summarise("Taux de consulations"= mean(taux_visites, na.rm = TRUE))

# Afficher les résultats dans le RMarkdown
kable(data_pop, caption = "Taux de consultations selon la taille de la commune")
```
En divisant les communes en trois groupes égaux (ou presque égaux) en fonction de la population totale, il ressort qu'en moyenne, plus la taille de la commune est importante plus le taux de consulations est élevé.


#### Taux de consultation et population âgée
```{r}
# Calculer la médiane
mediane <- median(data$nb_de_pers_agees_de_75_ans_ou_plus_2021, na.rm = TRUE)

# Créer des classes avec les bornes des intervalles
data_age <- data %>%
  mutate(population_agee_importante = case_when(
    nb_de_pers_agees_de_75_ans_ou_plus_2021 <= mediane ~ paste0("Non (<= ", round(mediane), ")"),
    TRUE ~ paste0("Oui (> ", round(mediane), ")")
  )) %>%
  group_by(population_agee_importante) %>%
  summarise(consultations_moyennes = mean(taux_visites, na.rm = TRUE))

# Afficher les résultats dans le RMarkdown
kable(data_age, caption = "Taux de consultations selon la population âgée")
```

```{r, fig.align='center', fig.cap="Relation entre taux de consultations et part des plus de 75 ans"}
# Charger la bibliothèque ggplot2
library(ggplot2)


ggplot(data = data, aes(x = part_des_pers_agees_de_75_ans_ou_2021 , y = taux_visites_19_ans_ou_plus)) +
  geom_point(color = "blue", size = 3) +          # Points bleus
  geom_smooth(method = "lm", se = TRUE, color = "red") +  # Droite de tendance (modèle linéaire)
  labs(
    x = "Part des plus de 75 ans",
    y = "Taux de consultation"
  ) +
  theme_minimal()

```

Les communes avec une population âgée importante (communes dont la population âgée de 75 ans ou plus est supérieure à la médiane) ont en moyenne un taux de consultations plus faible.

#### Taux de consultation et CSP

```{r, fig.align='center', fig.cap="Relations entre le taux de consultatiosn et certaines le nombre de certaines catégories socioprofessionnelle"}
# Charger les bibliothèques nécessaires
library(ggplot2)
library(gridExtra)

# Standardiser les variables pour prendre en compte l'effet de la taille des variables et rendre les comparaisons plus équitables
#data_csp <- data %>%
  #mutate(across(c(taux_visites, 13:20), scale))

data_csp <- data

# Créer les graphiques
plot1 <- ggplot(data_csp, aes(x = population_de_15_ans_ou_selon_la_csp_2021_agriculteurs_exploitants, y = taux_visites)) +
  geom_point() +
  labs(title = "", x = "Agriculteurs", y = "Taux de visites")

plot2 <- ggplot(data_csp, aes_string(x = names(data)[14], y = "taux_visites")) +
  geom_point() +
  labs(title = "", x = "Artisans", y = "Taux de visites")

plot3 <- ggplot(data_csp, aes_string(x = names(data)[15], y = "taux_visites")) +
  geom_point() +
  labs(title = "", x = "Cadres", y = "Taux de visites")

plot4 <- ggplot(data_csp, aes_string(x = names(data)[16], y = "taux_visites")) +
  geom_point() +
  labs(title = "", x = "Professions intermédiaires", y = "Taux de visites")

plot5 <- ggplot(data_csp, aes_string(x = names(data)[17], y = "taux_visites")) +
  geom_point() +
  labs(title = "", x = "Employés", y = "Taux de visites")

plot6 <- ggplot(data_csp, aes_string(x = names(data)[18], y = "taux_visites")) +
  geom_point() +
  labs(title = "", x = "Ouvriers", y = "Taux de visites")

plot7 <- ggplot(data_csp, aes_string(x = names(data)[19], y = "taux_visites")) +
  geom_point() +
  labs(title = "", x = "Retraités", y = "Taux de visites")

plot8 <- ggplot(data_csp, aes_string(x = names(data)[20], y = "taux_visites")) +
  geom_point() +
  labs(title = "", x = "Sans activites", y = "Taux de visites")

# Afficher les graphiques ensemble
grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, plot7, plot8, ncol = 2)
```
Aucune catégorie ne semble montrer une relation linéaire évidente avec le taux de visite. Par ailleurs, pour toutes les catégories socio-professionnelles, la majorité des communes se situent dans une plage de proportions faibles, ce qui limite la variabilité observable dans les relations. Une analyse statistique supplémentaire, comme le calcul de corrélations, serait nécessaire pour confirmer ou infirmer les relations observées visuellement.


#### Analyse de corrélation

Les résultats de la corrélation de Pearson sont consignées dans le tableau suivant :


```{r}
# Fonction de conversion
conversion_en_numeric <- function(data, columns) {
  resultat <- data %>%
    mutate(across(all_of(columns), as.numeric))
  return (resultat)
}
# Liste des variables à tester avec taux_de_consultation
variables <- c("population_municipale_2021_x", "part_des_pers_agees_de_75_ans_ou_2021", 
               "population_de_15_ans_ou_selon_la_csp_2021_retraites", "population_de_15_ans_ou_selon_la_csp_2021_ouvriers")

data <- conversion_en_numeric(data, variables)


# Initialiser un tableau pour stocker les résultats
resultats <- data.frame(Variable = character(), Correlation = numeric(), P_value = numeric())

# Calculer la corrélation pour chaque variable et tester la significativité
for (var in variables) {
  test <- cor.test(data$taux_visites_19_ans_ou_plus, data[[var]], method = "pearson")
  resultats <- rbind(resultats, data.frame(
    Variable = var,
    Correlation = test$estimate,
    P_value = test$p.value
  ))
}

# Format du tableau avec la significativité
resultats$Significatif <- ifelse(resultats$P_value < 0.05, "Oui", "Non")

# Afficher le tableau dans le RMarkdown
kable(resultats, caption = "Corrélations de Pearson entre le taux de consultation et les autres variables")
```


Les résultats nous montrent que le taux de consultation est positivement corrélé à la population ainsi qu'à celle de plus de 15 ans. Cependant la corrélation est faible. Par ailleurs, la corrélation est négative avec la part des personnes agées de plus de 75 ans. Cela dit, plus la part des plus de 75 ans augmente moins est le taux de consultations dans une commune. Cela peut vouloir dire que les personnes de plus de 75 ans sont ceux qui ne se consultent pas assez. 




### Autocorrélation 

L’autocorrélation spatiale est une mesure essentielle pour analyser la dépendance entre des observations géographiques. Dans notre étude nos données sont des données portant sur des communes. Ainsi il peut exister une dépendance entre nos taux de consultations du fait de la proximité des communes ou de l'appartenance à un même département ou région. Ainsi nous allons mesurer cette dépendance en évaluant l'autocorrélation spatiale. Dans ce contexte, **l’indice de Moran** est largement utilisé pour quantifier cette dépendance en fournissant une mesure globale de l’autocorrélation spatiale.

#### Définition de l’indice de Moran
L’indice de Moran (\(I\)) évalue la similitude des valeurs d’une variable entre différentes entités géographiques (par exemple, des communes) en fonction de leur proximité spatiale. Il se base sur la matrice de poids spatiale (\(W\)), qui définit les relations entre ces entités.

#### Formule de l’indice de Moran
La formule mathématique de l’indice de Moran est la suivante :

\[
I = \frac{n}{\sum_{i=1}^n \sum_{j=1}^n w_{ij}} \cdot \frac{\sum_{i=1}^n \sum_{j=1}^n w_{ij} (x_i - \bar{x})(x_j - \bar{x})}{\sum_{i=1}^n (x_i - \bar{x})^2}
\]

Où :

- \(n\) : Nombre total d’entités spatiales (Ici, le nombre de communes).

- \(x_i, x_j\) : Valeurs observées de la variable pour les entités \(i\) et \(j\) (Ici le taux de consultations)

- \(\bar{x}\) : Moyenne de la variable \(x\).

- \(w_{ij}\) : Poids spatial définissant la relation entre \(i\) et \(j\).

La matrice de \(W\) peut être constuit sur la base du voisinage entre les deux communes ou soit de la distance entre les deux communes. Dans le premier cas alors \(w_{ij}\) 
\(w_{ij} = 1\) si \(i\) et \(j\) sont voisins et \(w_{ij} = 0\) sinon. Dans le second cas \(w_{ij} = d_{ij}\). Nous allons dans notre cas utiliser une matrice de poids basée sur la distance, notamment celle d'Haversine. 

#### Matrice de poids basée sur la distance de Haversine

##### Définition de la distance de Haversine
La distance de Haversine est une mesure de la distance entre deux points sur une sphère, basée sur leurs coordonnées géographiques (\(latitude\) et \(longitude\)). Elle est particulièrement utile pour les données géographiques projetées sur une surface sphérique, comme la Terre.

#### Formule de la distance de Haversine
Si l'on considère deux points (\(i\)) et (\(j\)), la distance (\(d_{ij}\)) entre ces deux points sur la surface d'une sphère de rayon (\(r\)) est donnée par :

\[
 d_{ij} = 2r \cdot \arcsin\left(\sqrt{\sin^2\left(\frac{\phi_j - \phi_i}{2}\right) + \cos(\phi_i)\cos(\phi_j)\sin^2\left(\frac{\lambda_j - \lambda_i}{2}\right)}\right)
\]

Où :
- \(r\) : Rayon de la Terre (environ 6371 km).

- \(\phi_i, \phi_j\) : Latitudes des points \(i\) et \(j\) (en radians).

- \(\lambda_i, \lambda_j\) : Longitudes des points \(i\) et \(j\) (en radians).
Après calcul nous avons ces statistiques sur nos distances. 

```{r}
# Charger les bibliothèques nécessaires

#  (latitude, longitude)
library(spdep)     # Pour les fonctions de pondération spatiale et test de Moran
library(geosphere) # Pour les calculs de distances géodésiques

# Vérification que les colonnes latitude et longitude existent dans `data`
if (!("latitude" %in% names(data)) || !("longitude" %in% names(data))) {
  stop("Les colonnes 'latitude' et 'longitude' doivent exister dans la base de données.")
}

# Vérification des valeurs manquantes dans les coordonnées
if (anyNA(data$latitude) || anyNA(data$longitude)) {
  stop("Les colonnes 'latitude' et 'longitude' ne doivent pas contenir de valeurs manquantes.")
}

# Création de la matrice des coordonnées
coords <- data.frame(
  lat = data$latitude,
  lon = data$longitude
)

# Calcul des distances géodésiques (en mètres) avec la méthode de Vincenty
dist_matrix <- distm(coords, fun = distVincentySphere)/1000

# Gérer les distances nulles ou infinies
if (any(diag(dist_matrix) != 0)) {
  diag(dist_matrix) <- 0  # Auto-distance définie comme 0
}
if (any(is.infinite(dist_matrix))) {
  stop("La matrice des distances contient des valeurs infinies, vérifiez les coordonnées.")
}

# Résumé statistique de toutes les distances
distance_values <- as.vector(dist_matrix)
summary(distance_values)
```
Une visualtion de la densité de nos distance nous donne ceci. 
```{r fig.cap="Densité des distances"}
dist_df <- data.frame(Distance = as.vector(dist_matrix))
# Tracer la densité
ggplot(dist_df, aes(x = Distance)) +
  geom_density(fill = "blue", alpha = 0.4) +
  theme_minimal() +
  labs(x = "Distance", y = "Densité")

```


#### Construction de la matrice de poids

Pour construire la matrice de poids, nous avons alors suivi ces étapes. *

1. Calculer les distances de Haversine entre chaque paire d’entités.
2. Définir un seuil de distance maximale (\(d_{max}\)) :
   - Si \(d_{ij} < d_{max}\), \(w_{ij} = \frac{1}{d_{ij}}\).
   - Sinon, \(w_{ij} = 0\).
3. Normaliser les poids pour que chaque ligne de la matrice ait une somme égale à 1 :
\[
 w_{ij}^{norm} = \frac{w_{ij}}{\sum_{j} w_{ij}}.
\]

```{r}
# Créer la matrice de poids (inverse des distances)
weight_matrix <- 1 / dist_matrix
diag(weight_matrix) <- 0  # Aucun poids pour soi-même

# Gérer les cas où les distances sont nulles ou infinies
weight_matrix[is.infinite(weight_matrix)] <- 0

# Créer l'objet spatial de pondération
W <- mat2listw(weight_matrix, style = "W")

# Vérifier que la variable à analyser existe et ne contient pas de NA
if (!("taux_visites_19_ans_ou_plus" %in% names(data))) {
  stop("La colonne 'taux_visites_19_ans_ou_plus' doit exister dans la base de données.")
}
values <- data$taux_visites_19_ans_ou_plus

if (anyNA(values)) {
  stop("La colonne 'taux_visites_19_ans_ou_plus' ne doit pas contenir de valeurs manquantes.")
}

# Calcul de l'indice de Moran
moran_result <- moran.test(values, W, zero.policy = TRUE)

# Afficher les résultats
kable(moran_result[["estimate"]], caption = "Résultats du test de Moran")

```


Ainsi dans notre étude, nous avons trouvé un indice de Moran égale à `r moran_result[["estimate"]][["Moran I statistic"]]`. Le test nous a permi d'obtenir une p-value de `r moran_result[["p.value"]]`. Ce qui permet de conclure qu'il y a effectivement une autocorrélation positive et significative entre les communes selon leur taux de consultations. 



# Discussion

# Conclusion

# Références bibliographiques

# Annexes


\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{../cartes/nombre_de_consulatations}
	\caption{Carte du nombre de consultations par commune}
	\label{fig:figure}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{../cartes/taux_de_consultations}
	\caption{Carte du taux de consultations par commune}
	\label{fig:figure}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{../cartes/taux_de_consultations_plus_19_ans}
	\caption{Carte du taux de consultations par commune pour les plus de 19 ans}
	\label{fig:figure}
\end{figure}


