# Chargement des packages

library(stringi)
library(dplyr)
library(tidyverse)
library(summarytools)
library(gridExtra)
library(purrr)
library(skimr)

# Importation des donn√©es de d√©monstration

demo <- read.csv("data/demof2.csv", sep = ";", dec=",")
data <- read.csv("data/data.csv", sep = ",", dec=",")
View(demo)
str(demo)
names(demo)[names(demo) == "Libell√©"] <- "libelle_maj"
data = demo
## Fonction prenant en entr√©e un base et nettoie les noms des colonnes

nettoyer_noms_colonnes <- function(data){
  names(data) <- names(data) %>%
    stri_trans_general("Latin-ASCII") %>% # Suppression des accents
    gsub("\\s+", "_", .) %>% # Remplacement des espaces par des underscores
    gsub("\\.+", "_", .) %>% # Remplacement des points par des underscores
    tolower() # Conversion en minuscules
  return (data) 
}

## Nettoyage des colonnes de la base demo
demo <- nettoyer_noms_colonnes(demo)
names(demo)


# Fusion des bases et cr√©ation des varaiables

## Importation de la base generalise
generalise <- read.csv("data/generalise.csv", sep=";")
str(generalise)

## Importation de la base pour les lon et lat manquantes
donnees_manquantes <- read.csv(
  "data/communes_manquantes_latitudes_longitudes.csv", sep=";", dec=".")
str(donnees_manquantes)

donnees_manquantes$longitude <- donnees_manquantes$longitude %>%
  str_replace_all(",", "") %>%  # Supprime les virgules
  as.numeric()

## Nettoyage dans les noms des colonnes
generalise <- nettoyer_noms_colonnes(generalise)
donnees_manquantes <- nettoyer_noms_colonnes(donnees_manquantes)

## Fusion des bases
data <- demo %>% 
  inner_join(generalise, by ="code") %>%
  left_join(donnees_manquantes, by = "code") %>%
  mutate(
    longitude = ifelse(is.na(longitude.x), longitude.y, longitude.x),
    latitude = ifelse(is.na(latitude.x), latitude.y, latitude.x)
  ) %>%
  select(-longitude.x, -longitude.y, -latitude.x, -latitude.y)


nrow(demo)
nrow(generalise)
nrow(data)

## Filtrons les communes n'appartenant pas au d√©partement 97
data <- data %>% filter(departement != 97)



## Cr√©ation de la variable taux de visites
data <- data %>% 
  mutate(taux_visites = nb_visite/population_municipale_2021_x)

## Cr√©ation de la variabe taux de visites pour les plus de 19 ans
data <- data %>%
  mutate(pop_19_ans_ou_plus = pop_15_ans_ou_plus - pop_15_19_ans,
       taux_visites_19_ans_ou_plus = nb_visite / pop_19_ans_ou_plus)

summary(data$taux_visites)
summary(data$taux_visites_19_ans_ou_plus)

skim(data)

## Exportation de la base finale 
write.csv(data, "data/data.csv", row.names = FALSE)

## Statistiques descriptives sur le nombre de visite

summary(data$nb_visite)

ggplot(data) +
  aes(x = nb_visite) +
  geom_histogram(bins = 30L, fill = "gray") +
  theme_minimal() +
  ggtitle(label = "Distribution du nombre de visites par commune") +
  ylab("") +
  xlab("")


# Charger les biblioth√®ques n√©cessaires
library(ggplot2)
library(FactoMineR)
library(factoextra)
library(corrplot)
library(dplyr)

# Charger les donn√©es (remplace "Test.csv" par ton vrai fichier)
df <- read.csv("data/data.csv", sep = ",", dec=",")

# V√©rifier la structure des donn√©es
str(df)

# Conversion des colonnes en num√©rique si n√©cessaire
df_numeric <- df %>%
  select(where(is.numeric)) %>%  # S√©lectionne les colonnes num√©riques
  select(-nb_visite)             # Exclut la variable cible

# Standardiser les donn√©es pour l'ACP
df_scaled <- scale(df_numeric)

# Effectuer l'ACP
pca_result <- PCA(df_scaled, scale.unit=TRUE, graph=FALSE)

# Afficher le pourcentage de variance expliqu√©e
fviz_eig(pca_result, addlabels=TRUE, ylim=c(0,100))

# Afficher la contribution des variables aux deux premi√®res composantes principales
fviz_pca_var(pca_result, col.var="contrib", gradient.cols=c("blue", "red"), repel=TRUE)

# -------------------------------------------------
# üìå ANALYSE BIVARI√âE : Corr√©lation avec nb_visite
# -------------------------------------------------
# Ajouter la variable cible
df_numeric$nb_visite <- df$nb_visite

# Calculer les corr√©lations
cor_matrix <- cor(df_numeric, use="complete.obs")

# Trier les variables les plus corr√©l√©es avec nb_visite
cor_target <- sort(cor_matrix["nb_visite",], decreasing=TRUE)

# Afficher les 10 variables les plus corr√©l√©es avec nb_visite
print(cor_target[1:30])

# Visualisation des corr√©lations sous forme de heatmap
corrplot(cor_matrix, method="color", type="upper", tl.col="black", tl.srt=45)


# Charger les biblioth√®ques n√©cessaires
library(ggplot2)
library(dplyr)
library(tidyr)

colnames(df) <- gsub("homme_", "hommes_", colnames(df))
colnames(df) <- gsub("hommes_70_47$", "hommes_70_74", colnames(df))


# Charger les donn√©es (remplace "Test.csv" par ton fichier)
df <- read.csv("Test.csv", sep="\t", header=TRUE)

# S√©lectionner les colonnes de la pyramide des √¢ges
age_groups <- c("0_4", "5_9", "10_14", "15_19", "20_24", "25_29", "30_34", "35_39",
                "40_44", "45_49", "50_54", "55_59", "60_64", "65_69", "70_74",
                "75_79", "80_84", "85_89", "90_94", "95_plus")

# Restructurer les donn√©es pour la visualisation
hommes_vars <- intersect(colnames(df), paste0("hommes_", age_groups))
femmes_vars <- intersect(colnames(df), paste0("femmes_", age_groups))

# Cr√©er la pyramide des √¢ges avec les hommes d'abord, puis les femmes
pyramide <- data.frame(
  Age = rep(age_groups, 2),  # Liste tous les √¢ges d'abord pour les hommes, puis pour les femmes
  Sexe = c(rep("Homme", length(age_groups)), rep("Femme", length(age_groups))),
  Population = c(colSums(df[paste0("hommes_", age_groups)], na.rm=TRUE),
                 -colSums(df[paste0("femmes_", age_groups)], na.rm=TRUE))  # Femmes en n√©gatif
)



ggplot(pyramide, aes(x=Age, y=Population, fill=Sexe)) +
  geom_bar(stat="identity", width=0.8) +
  coord_flip() +  # Pour afficher en pyramide
  scale_y_continuous(labels = abs) +  # Afficher les valeurs absolues
  labs(title="Pyramide des √¢ges",
       x="Tranche d'√¢ge",
       y="Population",
       fill="Sexe") +
  theme_minimal() +
  scale_fill_manual(values=c("blue", "pink"))  # Couleurs pour Homme/Femme


## D'autres analyses

# Charger les biblioth√®ques n√©cessaires
library(ggplot2)
library(dplyr)
library(corrplot)

# S√©lectionner uniquement les variables d'int√©r√™t
variables_analyse <- c("taux_de_mortalite_annuel_moyen_2015_2021", 
                       "taux_de_natalite_annuel_moyen_2015_2021", 
                       "part_des_familles_sans_enf_de_de_25_ans_2021", 
                       "part_des_familles_avec_1_enf_de_de_25_ans_2021", 
                       "part_des_familles_avec_3_enf_ou_plus_de_de_25_ans_2021", 
                       "nb_visite")

df_analyse <- df[variables_analyse]
# Convertir toutes les colonnes en num√©rique
df_analyse <- df_analyse %>% mutate(across(everything(), as.numeric))

# V√©rifier les valeurs manquantes et les g√©rer si besoin
df_analyse <- na.omit(df_analyse)  

# Calculer les corr√©lations entre nb_visite et les autres variables
cor_matrix <- cor(df_analyse, use="complete.obs")

# Trier les corr√©lations par ordre d√©croissant
cor_target <- sort(cor_matrix["nb_visite",], decreasing=TRUE)

# Afficher le top des corr√©lations
print(cor_target)

# Visualiser les corr√©lations sous forme de barplot
ggplot(data = data.frame(Variable = names(cor_target), Correlation = cor_target), 
       aes(x = reorder(Variable, Correlation), y = Correlation, fill = Correlation)) +
  geom_bar(stat="identity") +
  coord_flip() +
  scale_fill_gradient2(low="blue", mid="white", high="red", midpoint=0) +
  labs(title="Corr√©lations entre le nombre de visite et les autres variables",
       x="Variables",
       y="Coefficient de corr√©lation") +
  theme_minimal()


################################################
### MORTALITE ET NATALITE########################"

# üìå V√©rification et conversion des variables avant de tracer
# V√©rification et conversion du taux de mortalit√©
if (class(df$taux_de_mortalite_annuel_moyen_2015_2021) != "numeric") {
  df$taux_de_mortalite_annuel_moyen_2015_2021 <- as.numeric(as.character(df$taux_de_mortalite_annuel_moyen_2015_2021))
}

# V√©rification et conversion du taux de natalit√©
if (class(df$taux_de_natalite_annuel_moyen_2015_2021) != "numeric") {
  df$taux_de_natalite_annuel_moyen_2015_2021 <- as.numeric(as.character(df$taux_de_natalite_annuel_moyen_2015_2021))
}

# üìå 1Ô∏è‚É£ Histogramme du taux de mortalit√©
p1 <- ggplot(df, aes(x = taux_de_mortalite_annuel_moyen_2015_2021)) +
  geom_histogram(bins = 30, fill = "red", alpha = 0.7, color = "black") +
  labs(title = "Distribution du taux de mortalit√© (2015-2021)", 
       x = "Taux de mortalit√© moyen", 
       y = "Nombre de communes") +
  theme_minimal()

# üìå 2Ô∏è‚É£ Histogramme du taux de natalit√©
p2 <- ggplot(df, aes(x = taux_de_natalite_annuel_moyen_2015_2021)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7, color = "black") +
  labs(title = "Distribution du taux de natalit√© (2015-2021)", 
       x = "Taux de natalit√© moyen", 
       y = "Nombre de communes") +
  theme_minimal()

# üìå 3Ô∏è‚É£ Nuage de points pour voir la relation entre mortalit√© et natalit√©
p3 <- ggplot(df, aes(x = taux_de_mortalite_annuel_moyen_2015_2021, 
                     y = taux_de_natalite_annuel_moyen_2015_2021)) +
  geom_point(alpha = 0.7, color = "purple") +
  geom_smooth(method = "lm", color = "black", linetype = "dashed") +  # Ajout d'une tendance lin√©aire
  labs(title = "Relation entre taux de mortalit√© et taux de natalit√©",
       x = "Taux de mortalit√© moyen (2015-2021)",
       y = "Taux de natalit√© moyen (2015-2021)") +
  theme_minimal()

# üìå 4Ô∏è‚É£ Courbes de densit√© pour mieux voir la distribution
p4 <- ggplot(df) +
  geom_density(aes(x = taux_de_mortalite_annuel_moyen_2015_2021, fill = "Mortalit√©"), alpha = 0.5, color = "red") +
  geom_density(aes(x = taux_de_natalite_annuel_moyen_2015_2021, fill = "Natalit√©"), alpha = 0.5, color = "blue") +
  labs(title = "Distribution des taux de mortalit√© et natalit√©", 
       x = "Taux",
       y = "Densit√©") +
  scale_fill_manual(values = c("Mortalit√©" = "red", "Natalit√©" = "blue")) +
  theme_minimal()

# üìå Affichage de tous les graphiques ensemble
library(gridExtra)
grid.arrange(p1, p2, p3, p4, ncol = 2)


################################################
### DISTANCES ########################


# Charger les biblioth√®ques n√©cessaires

#  (latitude, longitude)
library(spdep)     # Pour les fonctions de pond√©ration spatiale et test de Moran
library(geosphere) # Pour les calculs de distances g√©od√©siques

# V√©rification que les colonnes latitude et longitude existent dans `data`
if (!("latitude" %in% names(data)) || !("longitude" %in% names(data))) {
  stop("Les colonnes 'latitude' et 'longitude' doivent exister dans la base de donn√©es.")
}

# V√©rification des valeurs manquantes dans les coordonn√©es
if (anyNA(data$latitude) || anyNA(data$longitude)) {
  stop("Les colonnes 'latitude' et 'longitude' ne doivent pas contenir de valeurs manquantes.")
}

dist_df <- data.frame(Distance = as.vector(dist_matrix))
# Tracer la densit√©
ggplot(dist_df, aes(x = Distance)) +
  geom_density(fill = "blue", alpha = 0.4) +
  theme_minimal() +
  labs(x = "Distance", y = "Densit√©")


# Cr√©ation de la matrice des coordonn√©es
coords <- data.frame(
  lat = data$latitude,
  lon = data$longitude
)

# Calcul des distances g√©od√©siques (en m√®tres) avec la m√©thode de Vincenty
dist_matrix <- distm(coords, fun = distVincentySphere)/1000

# G√©rer les distances nulles ou infinies
if (any(diag(dist_matrix) != 0)) {
  diag(dist_matrix) <- 0  # Auto-distance d√©finie comme 0
}
if (any(is.infinite(dist_matrix))) {
  stop("La matrice des distances contient des valeurs infinies, v√©rifiez les coordonn√©es.")
}

# R√©sum√© statistique de toutes les distances
distance_values <- as.vector(dist_matrix)
summary(distance_values)


# Identification des paires de communes o√π la distance d√©passe 2000 km
indices <- which(dist_matrix > 2000, arr.ind = TRUE)
# Extraire la colonne des indices des lignes
ligne_indices <- indices[, 1]

# Supprimer les doublons
ligne_indices_unique <- unique(ligne_indices)

# Afficher les lignes uniques
print(ligne_indices_unique[1:7])

for (i in 1:nrow(indices)) {
  print(paste("Commune A: Commune", indices[i, 1], 
              "et Commune B: Commune", indices[i, 2], 
              "- Distance:", dist_matrix[indices[i, 1], indices[i, 2]], "km"))
}
# Affichage des r√©sultats
print(communes_lointaines)

https://datascienceplus.com/spatial-regression-in-r-part-1-spamm-vs-glmmtmb/

  
set.seed(123)

# Nombre de communes
n_communes <- 50  

# Variables explicatives (exemple r√©aliste)
TauxNatalite <- runif(n_communes, 10, 40)  # Taux de natalit√© entre 10 et 40 pour 1000
TauxMortalite <- runif(n_communes, 5, 15)  # Taux de mortalit√© entre 5 et 15 pour 1000
PctFamille1Enfant <- runif(n_communes, 20, 60)  # % de familles avec 1 enfant
PctFemmes_15_24 <- runif(n_communes, 5, 20)  
PctFemmes_25_34 <- runif(n_communes, 10, 30)  
PctHommes_15_24 <- runif(n_communes, 5, 20)  
PctHommes_25_34 <- runif(n_communes, 10, 30)  

# Effet spatial : Position des communes (latitude, longitude)
Latitude <- runif(n_communes, 14.5, 15.5)  
Longitude <- runif(n_communes, -17.5, -16.5)  

# G√©n√©ration du nombre total de consultations (mod√®le de Poisson)
lambda <- exp(0.05 * TauxNatalite - 0.02 * TauxMortalite + 
                0.01 * PctFamille1Enfant + 
                0.005 * (PctFemmes_15_24 + PctFemmes_25_34) + 
                0.005 * (PctHommes_15_24 + PctHommes_25_34) + 
                rnorm(n_communes, mean = 0, sd = 0.3))  

Consultations <- rpois(n_communes, lambda * 1000)  # Multipli√© pour obtenir des valeurs r√©alistes

# Cr√©ation du DataFrame
df <- data.frame(Commune = 1:n_communes, Consultations, TauxNatalite, TauxMortalite,
                   PctFamille1Enfant, PctFemmes_15_24, PctFemmes_25_34,
                   PctHommes_15_24, PctHommes_25_34, Latitude, Longitude)

head(df)  # Aper√ßu des donn√©es

library(glmmTMB)

model_glmmTMB <- glmmTMB(Consultations ~ TauxNatalite + TauxMortalite + 
                           PctFamille1Enfant + PctFemmes_15_24 + PctFemmes_25_34 + 
                           PctHommes_15_24 + PctHommes_25_34,
                         family = poisson, data = data)

summary(model_glmmTMB)

library(nlme)

model_nlme <- lme(fixed = Consultations ~ TauxNatalite + TauxMortalite + 
                    PctFamille1Enfant + PctFemmes_15_24 + PctFemmes_25_34 + 
                    PctHommes_15_24 + PctHommes_25_34, 
                  random = ~ 1 | Commune, 
                  data = data, 
                  correlation = corExp(form = ~ Longitude + Latitude))

summary(model_nlme)


library(INLA)

# Cr√©ation d'une matrice d‚Äôadjacence fictive
communes <- unique(df$Commune)
adjacency_matrix <- matrix(0, length(communes), length(communes)) 
diag(adjacency_matrix) <- 1  # Connexions basiques (modifiable selon le vrai r√©seau)

# Mod√®le spatial bay√©sien
model_inla <- inla(Consultations ~ TauxNatalite + TauxMortalite + 
                     PctFamille1Enfant + PctFemmes_15_24 + PctFemmes_25_34 + 
                     PctHommes_15_24 + PctHommes_25_34 +
                     f(Commune, model = "besag", graph = adjacency_matrix), 
                   family = "poisson", 
                   data = data)

summary(model_inla)

selected_variables = [
  'population_municipale_2021_x',  
  'nb_de_pers_agees_de_25_a_64_ans_2021_x', 
  'nb_de_pers_agees_de_65_ans_ou_2021', 
  'hommes_5_9', 
  'femmes_5-9', 
  'taux_de_mortalite_annuel_moyen_2015_2021', 
  'taux_de_natalite_annuel_moyen_2015_2021',
  'part_des_pers_agees_de_75_ans_ou_2021',
  'population_de_15_ans_ou_selon_la_csp_2021_cadres_et_professions_intellectuelles_superieures',
  'population_de_15_ans_ou_selon_la_csp_2021_employes', 
  'population_de_15_ans_ou_selon_la_csp_2021_ouvriers',  
  'part_des_familles_sans_enf_de_de_25_ans_2021', 
  'part_des_familles_avec_1_enf_de_de_25_ans_2021', 
  'part_des_familles_avec_3_enf_ou_plus_de_de_25_ans_2021', 
  'longitude', 'latitude' 
]

  